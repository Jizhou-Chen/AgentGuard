# AgentGuard: Re-purposing Tool Orchestrator of LLM Agents for Safety Evaluation of Orchestration

## System Overview

### AgentGuard encompasses 3 key components:
1. The LLM-based tool orchestrator within the
target agent under evaluation (referred to as "orchestrator" in the following context).
2. A Safety Constraint Expert
agent responsible for safety constraint generation (referred to as "constraint expert" in the following context).
3. A centralized prompting proxy agent to instruct the above two
components to perform testing and hardening.

### AgentGuard works in 4 key phases:
1. Unsafe Workflow Identification
    * The orchestrator is instructed to:
        * 1.1) Reflect on the tools given to it regarding their capabilities and usage, leveraging its internal knowledge.
        * 1.2) Evaluate the tools to identify possible.
unsafe workflows of calling these tools violating fundamental security principles in different task scenarios.
2. Unsafe Workflow Validation
   * For each identified unsafe workflow from the last phase, the orchestrator is then instructed to:
       * 2.1) Generate corresponding concrete orchestration plans as test cases (e.g., sequences of tool API calls) that realistically represent the workflow.
       * 2.2) Execute the test cases to validate that the identified workflows indeed can result in unsafe outcomes, leveraging its privileges to invoke the tools.
3. Safety Constraint Generation
   * For each validated unsafe workflow from the last phase, along with the test case, and the unsafe outcomes, the constraint expert is instructed to:
       * 3.1) Examine the observed unsafe outcomes from test case execution.
       * 3.2) Correlate them with the tool invocations in the test case to identify the root cause.
       * 3.3) Generate corresponding safety constraints (e.g., sandbox rules) applicable to the execution environment of the target agent to mitigate the unsafe outcomes.
4. Safety Constraint Validation
    * For each validated unsafe workflow:
        * 4.1) The constraints generated by the constraint expert from the last phase are applied to the execution environment.
        * 4.2) The orchestrator is instructed to re-execute the test case and check if the unsafe outcomes are mitigated. If so, the set of safety constraints generated for this unsafe workflow is validated and collected.

### Deliverable
The above phases progressively construct the evaluation report containing a list of _Task Scenario_, _Unsafe Workflow_, _Test Case_, and _Safety Constraints_ along with the validation status of _Unsafe Workflow_ and _Safety Constraints_.

![System Architecture](docs/system_diagram.png)

### Applications of the Report
* The validated safety constraints can be enforced to sandbox the behaviors of the agent for safer deployment.
* The difference in the occurrences of unsafe outcomes before and after applying the safety constraints can be used to quantitatively evaluate the safety improvement in every iteration.
* The identified unsafe workflows can be collected and serve as a benchmark corpus to measure the safety of similar agents in the same family (i.e., agents with similar functionalities and toolsets).
*  The validated unsafe workflows along with the test cases can be collected, processed, and shared as threat intelligence for risk detection and early prevention at orchestration time before the execution, hardening similar agents in the same family at deployment, besides the aforementioned safety constraints enforcement as the last line of defense.
*  **The reports collected over time can serve as samples to help train/finetune LLMs/LAMs for safer tool orchestration.**
*  ...



## Demo

**Clarification before we start:**

Due to the limitation of LLM in generating readily applicable SELinux rules (i.e., safety constraints. Detailed in our report), the pipeline often fails at this step, preventing AgentGuard from executing from end to end. Therefore, to demonstrate the expected functionalities of AgentGuard, we use real data generated from previous successful runs in each step to make this demo. Hopefully, we will be able to address this roadblocker by improving the Safety Constraint Expert Agent and provide a end-to-end demo in a live run.

[Demo Repository](https://github.com/samuelleecong/agentguard_demo)

## Status
* The current implementation is still a prototype under development. We wished to get all the issues resolved and present a perfect version to you all by the submission DDL, but in reality, we were unable to resolve all of them within the limited time frame with our best efforts :(
* The left issues mostly come from generating error-free SELinux rules, as mentioned above, and interaction with the Aider (the target agent for evaluation). However, all core components are functional.
* We collected some artifacts generated during our tests. If you are interested, we have some raw conversation history (with Aider) during our tests in `demos/.aider.chat.history.md`.
* Our current action items are: 1) resolve existing issues and 2) integrate improved design (those discussed in our report + we came up with after the hackathon) into the implementation after testing.


## Quick Start

1. **Create a virtual environment**:
    ```sh
    python3 -m venv venv
    ```

2. **Activate the virtual environment**:
    - On Windows:
        ```sh
        venv\Scripts\activate
        ```
    - On macOS and Linux:
        ```sh
        source venv/bin/activate
        ```

3. **Install dependencies**:
    ```sh
    pip install -r requirements.txt
    ```

## Usage

### Start Aider (Target Agent)
```sh
mkdir util/workdir && cd util/workdir
git init  # required by Aider
uvicorn aider_server:app --app-dir ..  # Do not use --reload as it may restart the session
```

### Start AgentGuard
```sh 
python3 -m src.agent_guard
```
